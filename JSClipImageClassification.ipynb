{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Japanese Stable CLIP による画像分類"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 環境設定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ライブラリーのインストール（Python 仮想環境の利用推奨）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U ftfy transformers sentencepiece gradio huggingface_hub accelerate protobuf ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ライブラリーのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, List\n",
    "import ftfy, html, re\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from huggingface_hub import notebook_login\n",
    "from transformers import AutoModel, AutoTokenizer, AutoImageProcessor, BatchFeature\n",
    "import gradio as gr\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### デモ用サンプル画像のダウンロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('samples'):\n",
    "    os.makedirs('samples')\n",
    "sample_images = [\n",
    "    ('https://upload.wikimedia.org/wikipedia/commons/thumb/8/8e/Hauskatze_langhaar.jpg/800px-Hauskatze_langhaar.jpg?20110412192053', 'samples/sample01.png'),\n",
    "    ('https://upload.wikimedia.org/wikipedia/commons/thumb/2/25/Black_barn_cat_-_Public_Domain_%282014_photo%3B_cropped_2022%29.jpg/800px-Black_barn_cat_-_Public_Domain_%282014_photo%3B_cropped_2022%29.jpg?20220510154737', 'samples/sample02.png'),\n",
    "    ('https://upload.wikimedia.org/wikipedia/commons/thumb/d/de/Airbus_A380.jpg/640px-Airbus_A380.jpg', 'samples/sample03.png'),\n",
    "    ('https://upload.wikimedia.org/wikipedia/commons/thumb/3/32/DeltaIVHeavy_NROL82_launch.jpg/640px-DeltaIVHeavy_NROL82_launch.jpg', 'samples/sample04.png'),\n",
    "    ('https://upload.wikimedia.org/wikipedia/commons/thumb/6/6f/011_The_lion_king_Tryggve_in_the_Serengeti_National_Park_Photo_by_Giles_Laurent.jpg/640px-011_The_lion_king_Tryggve_in_the_Serengeti_National_Park_Photo_by_Giles_Laurent.jpg', 'samples/sample05.png'),\n",
    "    ('https://upload.wikimedia.org/wikipedia/commons/thumb/0/06/Prunus_incisa_var._kinkiensis_%27Kumagaizakura%27_07.jpg/640px-Prunus_incisa_var._kinkiensis_%27Kumagaizakura%27_07.jpg', 'samples/sample06.png'),\n",
    "    ('https://upload.wikimedia.org/wikipedia/commons/thumb/c/ce/Camila_Moreno_2.jpg/640px-Camila_Moreno_2.jpg', 'samples/sample07.png'),\n",
    "    ('https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/USSRC_Rocket_Park.JPG/800px-USSRC_Rocket_Park.JPG', 'samples/sample08.png'),\n",
    "    ('https://upload.wikimedia.org/wikipedia/commons/f/fc/Kintamani_dog_white.jpg', 'samples/sample09.png'),\n",
    "    ('https://upload.wikimedia.org/wikipedia/commons/thumb/9/95/Man_biking_on_Recife_city.jpg/800px-Man_biking_on_Recife_city.jpg', 'samples/sample10.png')\n",
    "]\n",
    "\n",
    "for url, filename in sample_images:\n",
    "    if not os.path.exists(filename):\n",
    "        os.system(f'wget {url} -O {filename}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face へのログイン"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Japanese Stable CLIP モデルのロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_path = \"stabilityai/japanese-stable-clip-vit-l-16\"\n",
    "model = AutoModel.from_pretrained(model_path, trust_remote_code=True).eval().to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "processor = AutoImageProcessor.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## トークン化関数の定義"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### トークン化関数で使用するヘルパー関数の定義\n",
    "https://github.com/mlfoundations/open_clip/blob/main/src/open_clip/tokenizer.py#L65C8-L65C8 から引用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(text):\n",
    "    text = ftfy.fix_text(text)\n",
    "    text = html.unescape(html.unescape(text))\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def whitespace_clean(text):\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### トークン化関数の定義（分類のターゲットとなるカテゴリを表すテキストをトークン化する関数）\n",
    "CLIPのオリジナルコードから引用（https://github.com/openai/CLIP/blob/main/clip/clip.py#L195）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(\n",
    "    texts: Union[str, List[str]],\n",
    "    max_seq_len: int = 77,\n",
    "):\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    texts = [whitespace_clean(basic_clean(text)) for text in texts]\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        max_length=max_seq_len - 1,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "    # add bos token at first place\n",
    "    input_ids = [[tokenizer.bos_token_id] + ids for ids in inputs[\"input_ids\"]]\n",
    "    attention_mask = [[1] + am for am in inputs[\"attention_mask\"]]\n",
    "    position_ids = [list(range(0, len(input_ids[0])))] * len(texts)\n",
    "\n",
    "    return BatchFeature(\n",
    "        {\n",
    "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
    "            \"position_ids\": torch.tensor(position_ids, dtype=torch.long),\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## エンベディングを生成する関数を定義\n",
    "Japanese Stable CLIP を使ってテキストと画像のエンベディングを生成する関数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_text_embeddings(text):\n",
    "  if isinstance(text, str):\n",
    "    text = [text]\n",
    "  text = tokenize(texts=text)\n",
    "  text_features = model.get_text_features(**text.to(device))\n",
    "  text_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True)\n",
    "  del text\n",
    "  return text_features.cpu().detach()\n",
    "\n",
    "def compute_image_embeddings(image):\n",
    "  image = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "  with torch.no_grad():\n",
    "    image_features = model.get_image_features(**image)\n",
    "  image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\n",
    "  del image\n",
    "  return image_features.cpu().detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## デモ画面の準備"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 画像を分類するカテゴリー（起動時デフォルト）を定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    \"配達員\",\n",
    "    \"営業\",\n",
    "    \"消防士\",\n",
    "    \"救急隊員\",\n",
    "    \"自衛隊\",\n",
    "    \"スポーツ選手\",\n",
    "    \"警察官\",\n",
    "    \"動物\",\n",
    "    \"猫\",\n",
    "    \"犬\",\n",
    "    \"うさぎ\",\n",
    "    \"ライオン\",\n",
    "    \"虎\",\n",
    "    \"乗り物\",\n",
    "    \"飛行機\",\n",
    "    \"自動車\",\n",
    "    \"自転車\",\n",
    "    \"タクシー\",\n",
    "    \"バイク\",\n",
    "    \"ロケット\",\n",
    "    \"宇宙船\",\n",
    "    \"兵器\",\n",
    "    \"戦闘機\",\n",
    "    \"戦車\",\n",
    "    \"空母\",\n",
    "    \"護衛艦\",\n",
    "    \"潜水艦\",\n",
    "    \"ミサイル\",\n",
    "    \"植物\",\n",
    "    \"鳥\",\n",
    "    \"魚\",\n",
    "    \"花\",\n",
    "    \"樹木\",\n",
    "    \"人間\",\n",
    "    \"子供\",\n",
    "    \"老人\",\n",
    "    \"女性\",\n",
    "    \"男性\",\n",
    "    \"少女\",\n",
    "    \"少年\",\n",
    "    \"サッカー選手\",\n",
    "    \"アスリート\",\n",
    "    \"アニメキャラクター\",\n",
    "    \"ぬいぐるみ\",\n",
    "    \"妖精\",\n",
    "    \"エルフ\",\n",
    "    \"天使\",\n",
    "    \"魔性使い\",\n",
    "    \"魔法少女\",\n",
    "    \"歌姫\",\n",
    "    \"歌手\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### カテゴリを表すテキストのエンベディングを生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embeds = compute_text_embeddings(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## デモの実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TOP_K = 3 # 上位3つのカテゴリを表示\n",
    "\n",
    "def update_categories_fn(new_categories, state): # カテゴリーを更新ボタンが押された時に実行される関数\n",
    "    categories = [cat.strip() for cat in new_categories.split(',')]\n",
    "    text_embeds = compute_text_embeddings(categories)\n",
    "    state['categories'] = categories\n",
    "    state['text_embeds'] = text_embeds\n",
    "    return ', '.join(categories), state\n",
    "\n",
    "def update_categories_default_fn(state):\n",
    "    text_embeds = compute_text_embeddings(categories)\n",
    "    state['categories'] = categories\n",
    "    state['text_embeds'] = text_embeds\n",
    "    return ', '.join(categories), state\n",
    "\n",
    "def inference_fn(img, state): # 分類実行ボタンが押された時に実行される関数\n",
    "  if img is None:\n",
    "    return \"分類する画像をアップロードするか Examples から選択してください\"\n",
    "  text_embeds = state['text_embeds']\n",
    "  categories = state['categories']\n",
    "  num_categories = len(categories)\n",
    "  image_embeds = compute_image_embeddings(img) # 画像のエンベディングを生成\n",
    "  similarity = (100.0 * image_embeds @ text_embeds.T).softmax(dim=-1) # 画像のエンベディングとテキストのエンベディングの内積を計算\n",
    "  similarity = similarity[0].numpy().tolist() # 内積の結果をリストに変換\n",
    "  output_dict = {categories[i]: float(similarity[i]) for i in range(num_categories)} # カテゴリと確信度の辞書を生成\n",
    "  del image_embeds # 画像のエンベディングを削除\n",
    "  return output_dict # カテゴリと確信度の辞書を返す\n",
    "\n",
    "\n",
    "with gr.Blocks(title=\"Japanese Stable CLIP\") as demo: # Gradio による画面の作成\n",
    "    state = gr.State({\n",
    "            \"categories\": categories,\n",
    "            \"text_embeds\": text_embeds\n",
    "        })\n",
    "    gr.Markdown(\"# Japanese Stable CLIP による画像の分類（a.k.a. 画像によるテキストの検索）\")\n",
    "    with gr.Row():\n",
    "      with gr.Column():\n",
    "        inp = gr.Image(label=\"分類したい画像\", type=\"pil\", height=512)\n",
    "        btn = gr.Button(\"分類実行\")\n",
    "        \n",
    "      with gr.Column():\n",
    "        out = gr.Label(label=\"分類結果\", num_top_classes=TOP_K)\n",
    "        with gr.Accordion(\"分類先のカテゴリ一覧\", open=False):\n",
    "            categories_input = gr.Textbox(label=\"カテゴリー（カンマ区切り）\", value=\", \".join(categories), lines=4)\n",
    "            with gr.Row():\n",
    "              with gr.Column():\n",
    "                category_update_btn = gr.Button(\"カテゴリーを更新\")\n",
    "                category_update_btn.click(fn=update_categories_fn, inputs=[categories_input, state], outputs=[categories_input, state])\n",
    "              with gr.Column():\n",
    "                category_update_default_btn = gr.Button(\"デフォルトに戻す\")\n",
    "                category_update_default_btn.click(fn=update_categories_default_fn, inputs=[state], outputs=[categories_input, state])\n",
    "        with gr.Accordion(\"Japanese Stable CLIP について\", open=False):\n",
    "          with gr.Column():\n",
    "            gr.Markdown(\n",
    "            \"\"\"[Japanese Stable CLIP](https://huggingface.co/stabilityai/japanese-stable-clip-vit-l-16) is a [CLIP](https://arxiv.org/abs/2103.00020) model by [Stability AI](https://ja.stability.ai/).\n",
    "                  - Blog: https://ja.stability.ai/blog/japanese-stable-clip\n",
    "                  - Twitter: https://twitter.com/StabilityAI_JP\n",
    "                  - Discord: https://discord.com/invite/StableJP\"\"\"\n",
    "            )\n",
    "    with gr.Row():\n",
    "      examples = gr.Examples(\n",
    "        examples=[str(p) for p in Path(\"samples\").glob(\"*.png\")],\n",
    "        inputs=inp\n",
    "      )\n",
    "    btn.click(fn=inference_fn, inputs=[inp, state], outputs=[out])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.queue()\n",
    "    demo.launch(debug=True, share=True, inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
